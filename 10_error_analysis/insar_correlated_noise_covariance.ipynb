{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50560c2",
   "metadata": {},
   "source": [
    "# Estimate InSAR correlated noise covariance\n",
    "\n",
    "Gareth Funning, University of California, Riverside\n",
    "\n",
    "Noise in InSAR data is not 'white' $-$ which is to say, the noise in each pixel is not independent of the others. Rather, it is spatially correlated. Given what causes it, predominantly troposphere water vapor, this is not surprising. Water vapor bodies can be several kilometers to tens of kilometers across, and thus affect interferometric phase over similar length scales.\n",
    "\n",
    "While we do not currently have the means to remove all of the tropospheric water vapor signal from our interferograms, we can at least quantify the effect it may have on our data. Specifically, we can measure the amplitude of any water vapor signal and the length scale it correlates over in our interferograms. If we can estimate an average covariance-vs-distance relationship, we can use this to weight our data appropriately, or to simulate realistic correlated noise. The basic idea, from Ramon Hanssen in his 2001 book, is that one can estimate the covariance change with distance by first estimating the autocorrelation of an area of the interferogram (i.e. correlating it with itself, in two dimensions), and then radially averaging it. Then, we can fit a function, usually some kind of exponential function, to that average, and use that to calculate covariances between any pair of points.\n",
    "\n",
    "This specific implementation is based on [Tim Wright's 2003 paper on the Nenana Mountain, Alaska earthquake](https://doi.org/10.1029/2003GL018014), also used (and explained better, I think) in [my 2005 paper on the Bam, Iran earthquake](https://doi.org/10.1029/2004JB003338)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dd426",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "You need these things! Mostly mathy things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f19834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need these things\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "from scipy.fft import fft2, ifft2\n",
    "from scipy.fftpack import fftshift\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a375a6d",
   "metadata": {},
   "source": [
    "## 1. Inputs\n",
    "\n",
    "Which files do you want to apply this analysis to? We need an interferogram and a way (or ways) to mask out bad pixels. And paths to those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "ifgpath=\"/home/gareth/work/quakes/elazig/merged/\"                     # directory with your interferogram\n",
    "ifgname=\"filt_topophase.unw.geo\"                                      # interferogram file\n",
    "corname=\"topophase.cor.geo\"                                           # correlation file\n",
    "mskname=\"water.msk\"                                                   # water mask file (optional)\n",
    "use_water_mask=True\n",
    "\n",
    "# parameters\n",
    "wavel = 0.0555     # radar wavelength in m\n",
    "cor_thresh = 0.25  # correlation threshold between 0 (bad) and 1 (perfect)\n",
    "pixsize=30         # interferogram pixel size in m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4b9fc",
   "metadata": {},
   "source": [
    "## 2. Load in the data\n",
    "\n",
    "We can use gdal for this. We will also convert phase to displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data manually!\n",
    "\n",
    "# unwrapped interferogram\n",
    "ifgfile=gdal.Open(ifgpath+ifgname,gdal.GA_ReadOnly)   # open with gdal\n",
    "rb = ifgfile.GetRasterBand(2)                         # phase info is in band 2\n",
    "ifg = rb.ReadAsArray()                                # read it in as a number array\n",
    "\n",
    "# convert to meters\n",
    "ifg*=wavel/4/np.pi                # change values of sc.displacement in place\n",
    "\n",
    "# let's plot it to have a look\n",
    "### matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(10,10))        # new figure called \"ax1\"\n",
    "im = ax.imshow(ifg,origin='upper')             # plot displacements with origin at upper left (ISCE default)\n",
    "fig.colorbar(im)                               # plot a color bar!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74711c",
   "metadata": {},
   "source": [
    "## 3. Mask and crop the data\n",
    "\n",
    "Apply your water mask (if you have one), and mask out low correlation pixels. Then apply a crop (of your choice) $-$ make sure to choose an area that does not have your primary signal of interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44119cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify crop bounds (in image coordinates, e.g. from mdx, in pixels from the top left)\n",
    "# for the full scene, set minima to 0 and maxima to -1\n",
    "ymin=0\n",
    "ymax=3000\n",
    "xmin=0\n",
    "xmax=-1\n",
    "\n",
    "# correlation file\n",
    "corfile=gdal.Open(ifgpath+corname,gdal.GA_ReadOnly)   # open with gdal\n",
    "rb = corfile.GetRasterBand(2)                         # correlation info is in band 2\n",
    "cor = rb.ReadAsArray()                                # read it in as a number array\n",
    "       \n",
    "# apply the water mask, if you have one\n",
    "if use_water_mask==True:\n",
    "    # water mask file\n",
    "    mskfile=gdal.Open(ifgpath+mskname,gdal.GA_ReadOnly)  \n",
    "    rb = mskfile.GetRasterBand(1)                         # only one band here\n",
    "    msk = rb.ReadAsArray()\n",
    "    ifg[msk<1]=np.nan                                     # mask out water\n",
    "\n",
    "# convert displacements to meters and mask low correlation pixels\n",
    "ifg[cor<cor_thresh]=np.nan          # mask out low correlation\n",
    "\n",
    "# get metadata for the image: frame size, pixel size and reference point\n",
    "gt = corfile.GetGeoTransform()    # geotransform information\n",
    "nrows=corfile.RasterYSize\n",
    "\n",
    "# sort out the maximum crop size in the y direction\n",
    "if ymax==-1:\n",
    "    ymax=nrows\n",
    "    \n",
    "# crop the ifg\n",
    "cropifg=ifg[ymin:ymax,xmin:xmax]\n",
    "\n",
    "# and plot it\n",
    "fig, ax = plt.subplots(figsize=(10,6))        # new figure called \"ax1\"\n",
    "im = ax.imshow(cropifg,origin='upper')             # plot displacements with origin at upper left (ISCE default)\n",
    "fig.colorbar(im)                               # plot a color bar!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc657f49",
   "metadata": {},
   "source": [
    "## 4. Flatten your cropped interferogram\n",
    "\n",
    "Any ramp in the cropped data will be the largest correlated signal. We don't want to include that in our analysis! So we will flatten the cropped data by subtracting out a best-fitting plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a8a70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# flatten the interferogram crop\n",
    "\n",
    "# make meshgrids for x and y pixel numbers\n",
    "nrows,ncols = np.shape(cropifg)\n",
    "xx,yy=np.meshgrid(np.arange(0,ncols,1),np.arange(0,nrows,1))\n",
    "# set nulls to zero, make sparse and reshape to column\n",
    "xx = xx.astype('float')\n",
    "yy = yy.astype('float')\n",
    "\n",
    "# nan out the nan pixels in your grids\n",
    "xx[np.isnan(cropifg)]=np.nan\n",
    "yy[np.isnan(cropifg)]=np.nan\n",
    "\n",
    "# make vectors of pixel numbers and cropped ifg numbers\n",
    "xx1d = xx.ravel()\n",
    "xx1d = xx1d[~np.isnan(xx1d)]\n",
    "yy1d = yy.ravel()\n",
    "yy1d = yy1d[~np.isnan(yy1d)]\n",
    "cropifg1d = cropifg.ravel()\n",
    "cropifg1d = cropifg1d[~np.isnan(cropifg1d)]\n",
    "\n",
    "# invert for a best-fitting plane\n",
    "A = np.vstack((np.ones_like(xx1d),xx1d,yy1d)).T\n",
    "ATA=np.matmul(A.T,A)\n",
    "ATd=np.matmul(A.T,cropifg1d.T)\n",
    "planeparams=np.matmul(np.linalg.inv(ATA),ATd.T)\n",
    "\n",
    "# make your best-fitting plane and subtract it\n",
    "plane = planeparams[0]+xx*planeparams[1]+yy*planeparams[2] \n",
    "flatcropifg = cropifg-plane\n",
    "\n",
    "# and plot it\n",
    "fig, ax = plt.subplots(figsize=(10,6))         # new figure called \"ax1\"\n",
    "im = ax.imshow(flatcropifg,origin='upper')     # plot displacements with origin at upper left (ISCE default)\n",
    "fig.colorbar(im)                               # plot a color bar!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17103227",
   "metadata": {},
   "source": [
    "## 5. Calculate the autocorrelation of your crop\n",
    "\n",
    "The crux of the analysis is that we want to autocorrelate our data. We can do this either in the Fourier domain (fast) or in the spatial domain (slow). I strongly recommend (and have coded up) the Fourier approach $-$ which involves forward and inverse 2d Fourier transforms, plus some messing around with shifting the grid. After that, we will calculate some distances from the center of the autocorrelation grid to all of the other points on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abe275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# autocorrelation in the Fourier domain\n",
    "\n",
    "# the fft does not like nans\n",
    "flatcropifgzeros=flatcropifg\n",
    "flatcropifgzeros[np.isnan(flatcropifg)]=0\n",
    "\n",
    "# take the 2d fft, extract the phase spectrum, inverse 2d fft, shift, normalize\n",
    "fft_flatgrid = fft2(flatcropifgzeros)\n",
    "pspec = np.real(fft_flatgrid)**2 + np.imag(fft_flatgrid)**2\n",
    "autocorr_grid = ifft2(pspec)\n",
    "autocorr_grid = fftshift(np.real(autocorr_grid))/np.count_nonzero(flatcropifgzeros)\n",
    "\n",
    "xcent = np.floor(ncols/2)+1 # spectrally, the grid is not doubled in size\n",
    "ycent = np.floor(nrows/2)+1 \n",
    "\n",
    "# plot the autocorrelation grid\n",
    "fig, ax = plt.subplots(figsize=(10,6))                 # new figure called \"ax1\"\n",
    "im = ax.imshow(np.real(autocorr_grid),origin='upper')  # plot grid with origin at upper left (ISCE default)\n",
    "fig.colorbar(im)                                       # plot a color bar!\n",
    "plt.show()\n",
    "\n",
    "# make grid of distances from centre \n",
    "xx, yy = np.meshgrid(np.arange(0,ncols,1),np.arange(0,nrows,1))\n",
    "radial_dist = np.sqrt((xx-xcent)**2+(yy-ycent)**2)*pixsize;\n",
    "rd = np.ravel(radial_dist)\n",
    "acg = np.ravel(autocorr_grid)\n",
    "\n",
    "# cut the autocorrelation grid and distance vector in half\n",
    "rdlen=nrows+np.ceil(len(rd)/2) # only need 1st half of image (symmetry)\n",
    "rdlen=int(rdlen)\n",
    "\n",
    "rd = rd[0:rdlen] # only need 1st half of image (symmetry)\n",
    "acg= acg[0:rdlen]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5e527",
   "metadata": {},
   "source": [
    "## 6. Calculate the radial average of the autocorrelation and fit a function to it\n",
    "\n",
    "We are close to having a covariance vs distance relationship! We will calculate the radial average of the autocorrelation function, to get covariance as a function of distance, and then try a couple of different functions to fit to it. Exponential functions tend to fit okay, usually modifying them with a cosine function works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is our maximum autocorrelation?\n",
    "maxacg=np.max(acg)\n",
    "maxr = np.ceil(np.max(rd))\n",
    "print(\"maximum autocorrelation = {0:g} m^2\".format(maxacg))\n",
    "print(\"   corresponding to a standard deviation of = {0:f} m\".format(np.sqrt(maxacg)))\n",
    "\n",
    "print(' ')\n",
    "\n",
    "# calculate average cov vs dist profile\n",
    "\n",
    "# let's bin by twice the pixel size\n",
    "w = pixsize*2    # bin width\n",
    "\n",
    "# we should only fit a function to half of the autocorrelation grid (the shorter half)\n",
    "if (xcent<ycent):\n",
    "    maxdist = xcent*pixsize    \n",
    "else:\n",
    "    maxdist = ycent*pixsize\n",
    "    \n",
    "# truncate the radial distances and autocorrelations to this maximum distance\n",
    "rdtrunc=rd[rd<maxdist]\n",
    "acgtrunc=acg[rd<maxdist]\n",
    "\n",
    "# bin all the values by distance\n",
    "rdbin=np.int32(np.ceil(rdtrunc/w))    # classify values of rd according to bin number\n",
    "cvdav = np.zeros((max(rdbin),2))\n",
    "maxbin = np.max(rdbin)-1;\n",
    "\n",
    "# do the averaging, bin by bin\n",
    "for b in np.arange(0,maxbin+1,1):\n",
    "    cvdav[b,1] = np.mean(acgtrunc[np.where(rdbin==b+1)])\n",
    "    cvdav[b,0] = b*w\n",
    "\n",
    "# now, let's fit something to that\"\n",
    "\n",
    "# define a penalty function for an exponential fit\n",
    "def pendiffexp(params, cvdav):\n",
    "    a = cvdav[0,1]\n",
    "    b = params[0]\n",
    "    eff = cvdav[:,1]-a*np.exp(-b*cvdav[:,0])\n",
    "    f=np.linalg.norm(eff,2)\n",
    "    return f\n",
    "\n",
    "# define a penalty function for an exponential + cosine fit\n",
    "def pendiffexpcos(params, cvdav):\n",
    "    a = cvdav[0,1]\n",
    "    b = params[0]\n",
    "    c = params[1]\n",
    "    eff = cvdav[:,1]-a*np.exp(-b*cvdav[:,0])*np.cos(c*cvdav[:,0])\n",
    "    f=np.linalg.norm(eff,2)\n",
    "    return f\n",
    "\n",
    "# guess a trial solution\n",
    "exp_trial=np.array((0.00015))             # try something in the realm of 10-4 m^-1\n",
    "expcos_trial=np.array((0.0002, 0.0002))  # try something in the realm of 10-4 m^-1\n",
    "\n",
    "# use the minimize function to fit something (feel free to change the method)\n",
    "exp_solution=minimize(pendiffexp,exp_trial,cvdav,method = 'Nelder-Mead')\n",
    "a=cvdav[0,1]\n",
    "b=exp_solution.x[0]\n",
    "exp_graph=a*np.exp(-b*rd)\n",
    "print(\"exponential function, alpha = {0:g} m^-1\".format(b))\n",
    "print(\"   1/alpha = {0:f} m\".format(1/b))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "expcos_solution=minimize(pendiffexpcos,expcos_trial,cvdav)\n",
    "a=cvdav[0,1]\n",
    "b=expcos_solution.x[0]\n",
    "c=expcos_solution.x[1]\n",
    "expcos_graph=a*np.exp(-b*rd)*np.cos(c*rd)\n",
    "print(\"exponential cosine function, alpha = {0:g} m^-1, beta = {1:g} m^-1\".format(b,c))\n",
    "print(\"   1/alpha = {0:f} m\".format(1/b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da16c9",
   "metadata": {},
   "source": [
    "## 7. Plot the results!\n",
    "\n",
    "With all that done, we can plot the results $-$ the range of autocorrelation values, the radial average, the different functional fits. We can also plot the e-folding value $-$ (1/e)\\* the maximum covariance $-$ and the corresponding e-folding wavelength (the distance over which the covariance drops to the e-folding value). See which function seems to represent the average covariance-distance relationship the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the outcome!\n",
    "plt.scatter(rd,acg,c='lightgray')\n",
    "plt.plot(cvdav[:,0], cvdav[:,1],c='black',label='average')  # Plot the chart\n",
    "plt.plot(rd, exp_graph,c='red',label='exp')  # Plot the chart\n",
    "plt.plot(rd, expcos_graph,c='blue',label='expcos')  # Plot the chart\n",
    "\n",
    "# mark on the e-folding value\n",
    "efold=maxacg/np.exp(1)\n",
    "plt.plot([0,maxr],[efold,efold],c='dimgray',ls='--')\n",
    "\n",
    "# mark on the e-folding wavelength\n",
    "idx = (np.abs(cvdav[:,1]-efold)).argmin()\n",
    "plt.plot([cvdav[idx,0],cvdav[idx,0]],[np.min(acg),maxacg],c='dimgray',ls='--')\n",
    "print('e-folding value: {0:g} m^2, e-folding distance: {1:f} m'.format(efold,cvdav[idx,0]))\n",
    "\n",
    "plt.xlabel(\"distance (m)\")  \n",
    "plt.ylabel(\"covariance (m^2)\")  \n",
    "plt.title(\"max covariance: {0:g} m^2\".format(maxacg))\n",
    "plt.legend()\n",
    "plt.show()  # display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a00c57",
   "metadata": {},
   "source": [
    "Remember those numbers as we will need them later on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef9138",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "Hanssen, R. F., 2001, Radar interferometry: Data interpretation and error analysis, Kluwer, Dordrecht, 308 pp.\n",
    "\n",
    "Wright, T. J., Z. Lu and C. Wicks, 2003, Source model for the $M_w$ 6.7, 23 October 2002, Nenana Mountain\n",
    "Earthquake (Alaska) from InSAR, Geophys. Res. Lett., 30, 1974, https://doi.org/10.1029/2003GL018014\n",
    "\n",
    "Funning, G. J., B. Parsons, T. J. Wright, J. A. Jackson and E. J. Fielding, 2005, Surface displacements and source parameters of the 2003 Bam, Iran earthquake from Envisat Advanced Synthetic Aperture Radar imagery, J. Geophys. Res. Solid Earth, 110 (B9), B09406, https://doi.org/10.1029/2004JB003338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a9544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
